{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tensorflow_datasets\n!pip install -q -U tensorflow-text tensorflow","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:30:48.529289Z","iopub.execute_input":"2023-06-17T10:30:48.529666Z","iopub.status.idle":"2023-06-17T10:31:17.616268Z","shell.execute_reply.started":"2023-06-17T10:30:48.529635Z","shell.execute_reply":"2023-06-17T10:31:17.614738Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import collections\nimport os\nimport pathlib\nimport re\nimport string\nimport sys\nimport tempfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text\nimport tensorflow as tf\nfrom tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-17T10:31:17.619889Z","iopub.execute_input":"2023-06-17T10:31:17.621160Z","iopub.status.idle":"2023-06-17T10:31:30.141554Z","shell.execute_reply.started":"2023-06-17T10:31:17.621108Z","shell.execute_reply":"2023-06-17T10:31:30.140243Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.get_logger().setLevel('ERROR')\n# pwd = pathlib.Path.cwd()\npwd = os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:30.142915Z","iopub.execute_input":"2023-06-17T10:31:30.143640Z","iopub.status.idle":"2023-06-17T10:31:30.149736Z","shell.execute_reply.started":"2023-06-17T10:31:30.143605Z","shell.execute_reply":"2023-06-17T10:31:30.148554Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Downloading the Dataset","metadata":{}},{"cell_type":"code","source":"examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n                               as_supervised=True)\ntrain_examples, val_examples = examples['train'], examples['validation']","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:30.152209Z","iopub.execute_input":"2023-06-17T10:31:30.152560Z","iopub.status.idle":"2023-06-17T10:31:42.431390Z","shell.execute_reply.started":"2023-06-17T10:31:30.152528Z","shell.execute_reply":"2023-06-17T10:31:42.430215Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 124.94 MiB (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff063b44f36b4313b451ed1d128fbff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f352338b103f456c911ecaf4f9e281cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de986b252c04605b7cf5f52d323c601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/51785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteYO6H3Q/ted_hrlr_translate-trai…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...:   0%|          | 0/1193 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteYO6H3Q/ted_hrlr_translate-vali…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/1803 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteYO6H3Q/ted_hrlr_translate-test…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"for pt, en in train_examples.take(1):\n    print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n    print(\"English: \", en.numpy().decode('utf-8'))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:42.432781Z","iopub.execute_input":"2023-06-17T10:31:42.433177Z","iopub.status.idle":"2023-06-17T10:31:42.564143Z","shell.execute_reply.started":"2023-06-17T10:31:42.433147Z","shell.execute_reply":"2023-06-17T10:31:42.562969Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\nEnglish:  and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n","output_type":"stream"}]},{"cell_type":"code","source":"train_en = train_examples.map(lambda pt, en : en)\ntrain_pt = train_examples.map(lambda pt, en : pt)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:42.565798Z","iopub.execute_input":"2023-06-17T10:31:42.568001Z","iopub.status.idle":"2023-06-17T10:31:42.618364Z","shell.execute_reply.started":"2023-06-17T10:31:42.567955Z","shell.execute_reply":"2023-06-17T10:31:42.617342Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Generating the vocabulary","metadata":{"execution":{"iopub.status.busy":"2023-06-15T17:07:04.707055Z","iopub.execute_input":"2023-06-15T17:07:04.707496Z","iopub.status.idle":"2023-06-15T17:07:04.788883Z","shell.execute_reply.started":"2023-06-15T17:07:04.707460Z","shell.execute_reply":"2023-06-15T17:07:04.787064Z"}}},{"cell_type":"code","source":"bert_tokenizer_params = dict(lower_case = True)\nreserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n\nbert_vocab_args = dict(\n    # The target vocabulary size\n    vocab_size = 8000,\n    # Reserved tokens that must be included in the vocabulary\n    reserved_tokens=reserved_tokens,\n    # Arguments for `text.BertTokenizer`\n    bert_tokenizer_params=bert_tokenizer_params,\n    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n    learn_params={},\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:42.619895Z","iopub.execute_input":"2023-06-17T10:31:42.620884Z","iopub.status.idle":"2023-06-17T10:31:42.626648Z","shell.execute_reply.started":"2023-06-17T10:31:42.620848Z","shell.execute_reply":"2023-06-17T10:31:42.625485Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\npt_vocab = bert_vocab.bert_vocab_from_dataset(\n    train_pt.batch(1000).prefetch(2),\n    **bert_vocab_args\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:31:42.628412Z","iopub.execute_input":"2023-06-17T10:31:42.628724Z","iopub.status.idle":"2023-06-17T10:34:52.783742Z","shell.execute_reply.started":"2023-06-17T10:31:42.628698Z","shell.execute_reply":"2023-06-17T10:34:52.782279Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CPU times: user 3min 11s, sys: 2.59 s, total: 3min 13s\nWall time: 3min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(pt_vocab[:10])\nprint(pt_vocab[100:120])\nprint(pt_vocab[1000:1010])\nprint(pt_vocab[-10:])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:34:52.785641Z","iopub.execute_input":"2023-06-17T10:34:52.786054Z","iopub.status.idle":"2023-06-17T10:34:52.792752Z","shell.execute_reply.started":"2023-06-17T10:34:52.786013Z","shell.execute_reply":"2023-06-17T10:34:52.791593Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao', 'me', 'pessoas', 'tem', 'ou', 'porque', 'quando', 'ao', 'ser', 'fazer', 'dos']\n['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nen_vocab = bert_vocab.bert_vocab_from_dataset(\n    train_en.batch(1000).prefetch(2),\n    **bert_vocab_args\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:34:52.796749Z","iopub.execute_input":"2023-06-17T10:34:52.797119Z","iopub.status.idle":"2023-06-17T10:37:01.816833Z","shell.execute_reply.started":"2023-06-17T10:34:52.797088Z","shell.execute_reply":"2023-06-17T10:37:01.815379Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CPU times: user 2min 10s, sys: 842 ms, total: 2min 11s\nWall time: 2min 9s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(en_vocab[:10])\nprint(en_vocab[100:110])\nprint(en_vocab[1000:1010])\nprint(en_vocab[-10:])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:01.818490Z","iopub.execute_input":"2023-06-17T10:37:01.818868Z","iopub.status.idle":"2023-06-17T10:37:01.825352Z","shell.execute_reply.started":"2023-06-17T10:37:01.818838Z","shell.execute_reply":"2023-06-17T10:37:01.824455Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n['as', 'all', 'at', 'one', 'people', 're', 'like', 'if', 'our', 'from']\n['choose', 'consider', 'extraordinary', 'focus', 'generation', 'killed', 'patterns', 'putting', 'scientific', 'wait']\n['##_', '##`', '##ย', '##ร', '##อ', '##–', '##—', '##’', '##♪', '##♫']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Writing vocabulary files for both languages","metadata":{}},{"cell_type":"code","source":"%%time\ndef write_vocab_file(filepath, vocab):\n  with open(filepath, 'w') as f:\n    for token in vocab:\n      print(token, file=f)\n    \nwrite_vocab_file('pt_vocab.txt', pt_vocab)\nwrite_vocab_file('en_vocab.txt', en_vocab)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:01.826536Z","iopub.execute_input":"2023-06-17T10:37:01.827311Z","iopub.status.idle":"2023-06-17T10:37:01.855456Z","shell.execute_reply.started":"2023-06-17T10:37:01.827269Z","shell.execute_reply":"2023-06-17T10:37:01.854637Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CPU times: user 11 ms, sys: 1 ms, total: 12 ms\nWall time: 11.4 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Build the tokenizer","metadata":{}},{"cell_type":"code","source":"pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\nen_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:01.856567Z","iopub.execute_input":"2023-06-17T10:37:01.857462Z","iopub.status.idle":"2023-06-17T10:37:01.881516Z","shell.execute_reply.started":"2023-06-17T10:37:01.857416Z","shell.execute_reply":"2023-06-17T10:37:01.880449Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for pt_examples, en_examples in train_examples.batch(3).take(1):\n  for ex in en_examples:\n    print(ex.numpy().decode('utf-8'))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:01.882883Z","iopub.execute_input":"2023-06-17T10:37:01.883431Z","iopub.status.idle":"2023-06-17T10:37:01.944685Z","shell.execute_reply.started":"2023-06-17T10:37:01.883400Z","shell.execute_reply":"2023-06-17T10:37:01.943800Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\nbut what if it were active ?\nbut they did n't test for curiosity .\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize the examples -> (batch, word, word-piece)\ntoken_batch = en_tokenizer.tokenize(en_examples)\n# Merge the word and word-piece axes -> (batch, tokens)\ntoken_batch = token_batch.merge_dims(-2,-1)\n\nfor ex in token_batch.to_list():\n  print(ex)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:01.945973Z","iopub.execute_input":"2023-06-17T10:37:01.946475Z","iopub.status.idle":"2023-06-17T10:37:02.032244Z","shell.execute_reply.started":"2023-06-17T10:37:01.946445Z","shell.execute_reply":"2023-06-17T10:37:02.030878Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15]\n[87, 90, 107, 76, 129, 1852, 30]\n[87, 83, 149, 50, 9, 56, 664, 85, 2512, 15]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Lookup each token id in the vocabulary.\ntxt_tokens = tf.gather(en_vocab, token_batch)\n# Join with spaces.\ntf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:02.034174Z","iopub.execute_input":"2023-06-17T10:37:02.034919Z","iopub.status.idle":"2023-06-17T10:37:02.128923Z","shell.execute_reply.started":"2023-06-17T10:37:02.034875Z","shell.execute_reply":"2023-06-17T10:37:02.127779Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve search ##ability , you actually take away the one advantage of print , which is s ##ere ##nd ##ip ##ity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"},"metadata":{}}]},{"cell_type":"markdown","source":" You can see that in the first example the words \"searchability\" and \"serendipity\" have been decomposed into \"search ##ability\" and \"s ##ere ##nd ##ip ##ity\". Magic of Sub word Tokenizer!","metadata":{}},{"cell_type":"markdown","source":"To re-assemble words from the extracted tokens, we use the BertTokenizer.detokenize method:","metadata":{}},{"cell_type":"code","source":"words = en_tokenizer.detokenize(token_batch)\ntf.strings.reduce_join(words, separator=' ', axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:02.130336Z","iopub.execute_input":"2023-06-17T10:37:02.130640Z","iopub.status.idle":"2023-06-17T10:37:02.220545Z","shell.execute_reply.started":"2023-06-17T10:37:02.130614Z","shell.execute_reply":"2023-06-17T10:37:02.219367Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)>"},"metadata":{}}]},{"cell_type":"code","source":"START = tf.argmax(tf.constant(reserved_tokens) == '[START]') # get start index from reserved tokens\nEND = tf.argmax(tf.constant(reserved_tokens) == '[END]') # get end index from reserved tokens\n\ndef add_start_end(ragged):\n    count = ragged.bounding_shape()[0]\n    starts = tf.fill([count, 1], START)\n    ends = tf.fill([count, 1], END)\n    return tf.concat([starts, ragged, ends], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:02.221991Z","iopub.execute_input":"2023-06-17T10:37:02.222333Z","iopub.status.idle":"2023-06-17T10:37:02.240116Z","shell.execute_reply.started":"2023-06-17T10:37:02.222304Z","shell.execute_reply":"2023-06-17T10:37:02.238658Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"words = en_tokenizer.detokenize(add_start_end(token_batch))\ntf.strings.reduce_join(words, separator = \" \", axis = -1, )","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:37:02.241641Z","iopub.execute_input":"2023-06-17T10:37:02.242154Z","iopub.status.idle":"2023-06-17T10:37:02.308731Z","shell.execute_reply.started":"2023-06-17T10:37:02.242112Z","shell.execute_reply":"2023-06-17T10:37:02.307541Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'[START] and when you improve searchability , you actually take away the one advantage of print , which is serendipity . [END]',\n       b'[START] but what if it were active ? [END]',\n       b\"[START] but they did n ' t test for curiosity . [END]\"],\n      dtype=object)>"},"metadata":{}}]},{"cell_type":"code","source":"def cleanup_text(reserved_tokens, token_txt):\n  # Drop the reserved tokens, except for \"[UNK]\".\n  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n  bad_token_re = \"|\".join(bad_tokens)\n\n  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n\n  # Join them into strings.\n  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:50:13.269575Z","iopub.execute_input":"2023-06-17T10:50:13.270154Z","iopub.status.idle":"2023-06-17T10:50:13.278466Z","shell.execute_reply.started":"2023-06-17T10:50:13.270120Z","shell.execute_reply":"2023-06-17T10:50:13.276802Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(en_examples.numpy())\nprint()\nprint()\ntoken_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\nwords = en_tokenizer.detokenize(token_batch)\nprint(words)\nprint()\nprint()\ncleanup_text(reserved_tokens, words).numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T10:52:26.255305Z","iopub.execute_input":"2023-06-17T10:52:26.255747Z","iopub.status.idle":"2023-06-17T10:52:26.323287Z","shell.execute_reply.started":"2023-06-17T10:52:26.255717Z","shell.execute_reply":"2023-06-17T10:52:26.322084Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n b'but what if it were active ?' b\"but they did n't test for curiosity .\"]\n\n\n<tf.RaggedTensor [[b'and', b'when', b'you', b'improve', b'searchability', b',', b'you',\n  b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of',\n  b'print', b',', b'which', b'is', b'serendipity', b'.']              ,\n [b'but', b'what', b'if', b'it', b'were', b'active', b'?'],\n [b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity',\n  b'.']                                                                    ]>\n\n\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .',\n       b'but what if it were active ?',\n       b\"but they did n ' t test for curiosity .\"], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"The following code block builds a CustomTokenizer class to contain the text.BertTokenizer instances, the custom logic, and the @tf.function wrappers required for export.","metadata":{}},{"cell_type":"code","source":"class CustomTokenizer(tf.Module):\n  def __init__(self, reserved_tokens, vocab_path):\n    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n    self._reserved_tokens = reserved_tokens\n    self._vocab_path = tf.saved_model.Asset(vocab_path)\n\n    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n    self.vocab = tf.Variable(vocab)\n\n    ## Create the signatures for export:   \n\n    # Include a tokenize signature for a batch of strings. \n    self.tokenize.get_concrete_function(\n        tf.TensorSpec(shape=[None], dtype=tf.string))\n\n    # Include `detokenize` and `lookup` signatures for:\n    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n    #   * `RaggedTensors` with shape [batch, tokens]\n    self.detokenize.get_concrete_function(\n        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n    self.detokenize.get_concrete_function(\n          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n\n    self.lookup.get_concrete_function(\n        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n    self.lookup.get_concrete_function(\n          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n\n    # These `get_*` methods take no arguments\n    self.get_vocab_size.get_concrete_function()\n    self.get_vocab_path.get_concrete_function()\n    self.get_reserved_tokens.get_concrete_function()\n\n  @tf.function\n  def tokenize(self, strings):\n    enc = self.tokenizer.tokenize(strings)\n    # Merge the `word` and `word-piece` axes.\n    enc = enc.merge_dims(-2,-1)\n    enc = add_start_end(enc)\n    return enc\n\n  @tf.function\n  def detokenize(self, tokenized):\n    words = self.tokenizer.detokenize(tokenized)\n    return cleanup_text(self._reserved_tokens, words)\n\n  @tf.function\n  def lookup(self, token_ids):\n    return tf.gather(self.vocab, token_ids)\n\n  @tf.function\n  def get_vocab_size(self):\n    return tf.shape(self.vocab)[0]\n\n  @tf.function\n  def get_vocab_path(self):\n    return self._vocab_path\n\n  @tf.function\n  def get_reserved_tokens(self):\n    return tf.constant(self._reserved_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:11:51.198086Z","iopub.execute_input":"2023-06-17T11:11:51.198534Z","iopub.status.idle":"2023-06-17T11:11:51.214415Z","shell.execute_reply.started":"2023-06-17T11:11:51.198499Z","shell.execute_reply":"2023-06-17T11:11:51.213479Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizers = tf.Module()\ntokenizers.pt = CustomTokenizer(reserved_tokens, '/kaggle/working/pt_vocab.txt')\ntokenizers.en = CustomTokenizer(reserved_tokens, '/kaggle/working/en_vocab.txt')","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:11:51.719603Z","iopub.execute_input":"2023-06-17T11:11:51.720607Z","iopub.status.idle":"2023-06-17T11:11:55.533322Z","shell.execute_reply.started":"2023-06-17T11:11:51.720566Z","shell.execute_reply":"2023-06-17T11:11:55.532145Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert_subword_tokenizer_pt_en_converter'\ntf.saved_model.save(tokenizers, model_name)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:13:25.191835Z","iopub.execute_input":"2023-06-17T11:13:25.192294Z","iopub.status.idle":"2023-06-17T11:13:25.827151Z","shell.execute_reply.started":"2023-06-17T11:13:25.192263Z","shell.execute_reply":"2023-06-17T11:13:25.825687Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"reloaded_tokenizers = tf.saved_model.load(model_name)\nreloaded_tokenizers.en.get_vocab_size().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:14:17.698204Z","iopub.execute_input":"2023-06-17T11:14:17.698604Z","iopub.status.idle":"2023-06-17T11:14:19.349562Z","shell.execute_reply.started":"2023-06-17T11:14:17.698572Z","shell.execute_reply":"2023-06-17T11:14:19.346868Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"7010"},"metadata":{}}]},{"cell_type":"code","source":"tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\nprint(tokens.numpy())\nprint()\nprint()\ntext_tokens = reloaded_tokenizers.en.lookup(tokens)\nprint(text_tokens)\nprint()\nprint()\nround_trip = reloaded_tokenizers.en.detokenize(tokens)\nprint(round_trip.numpy()[0].decode('utf-8'))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:17:16.717996Z","iopub.execute_input":"2023-06-17T11:17:16.718414Z","iopub.status.idle":"2023-06-17T11:17:16.879079Z","shell.execute_reply.started":"2023-06-17T11:17:16.718382Z","shell.execute_reply":"2023-06-17T11:17:16.877842Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"[[   2 4006 2358  687 1192 2365    4    3]]\n\n\n<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n  b'[END]']]>\n\n\nhello tensorflow !\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r {model_name}.zip {model_name}","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:18:45.601595Z","iopub.execute_input":"2023-06-17T11:18:45.602075Z","iopub.status.idle":"2023-06-17T11:18:46.817104Z","shell.execute_reply.started":"2023-06-17T11:18:45.602040Z","shell.execute_reply":"2023-06-17T11:18:46.815680Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"  adding: bert_subword_tokenizer_pt_en_converter/ (stored 0%)\n  adding: bert_subword_tokenizer_pt_en_converter/fingerprint.pb (stored 0%)\n  adding: bert_subword_tokenizer_pt_en_converter/saved_model.pb (deflated 91%)\n  adding: bert_subword_tokenizer_pt_en_converter/variables/ (stored 0%)\n  adding: bert_subword_tokenizer_pt_en_converter/variables/variables.data-00000-of-00001 (deflated 51%)\n  adding: bert_subword_tokenizer_pt_en_converter/variables/variables.index (deflated 33%)\n  adding: bert_subword_tokenizer_pt_en_converter/assets/ (stored 0%)\n  adding: bert_subword_tokenizer_pt_en_converter/assets/en_vocab.txt (deflated 54%)\n  adding: bert_subword_tokenizer_pt_en_converter/assets/pt_vocab.txt (deflated 57%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!du -h *.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-17T11:18:52.074460Z","iopub.execute_input":"2023-06-17T11:18:52.074981Z","iopub.status.idle":"2023-06-17T11:18:53.214718Z","shell.execute_reply.started":"2023-06-17T11:18:52.074936Z","shell.execute_reply":"2023-06-17T11:18:53.213254Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"172K\tbert_subword_tokenizer_pt_en_converter.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}